{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9ac38f-6f77-4156-85d2-85a839877028",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Teach an agent **how good an action is in a state** by updating a table of values.\n",
    "\n",
    "\n",
    "1. Agent states and actions\n",
    "    - List of agent actions\n",
    "    - List of agent states\n",
    "2. Initialise Q Table and learning paramaters\n",
    "    - learning_rate : How fast values change \n",
    "    - discount_factor : How much future rewards matter\n",
    "    - exploratory_probability\n",
    "3. work\n",
    "    1. get current agent state from input data\n",
    "    2. decide action to take\n",
    "        - random state if exploratory probability condition, else, the action that has the highest value in the Q-table\n",
    "    3. get the reward for the action taken\n",
    "    4. update the q-table\n",
    "\n",
    ">`q_predict = q_table[state, action]`  \n",
    ">`q_target = reward + discount_factor * np.max(q_table[next_state])`  \n",
    ">`q_table[state, action] += learning_rate * (q_target - q_predict)`\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb256426-94bb-4822-aad1-14b6fe47949a",
   "metadata": {},
   "source": [
    "# Gymnasium \n",
    "\n",
    "Gymnasium provides an API for all single agent reinforcement learning environments, with implementations of common environments.\n",
    "\n",
    "https://gymnasium.farama.org/\n",
    "\n",
    "An API standard for reinforcement learning with a diverse collection of reference environments. e.g\n",
    "\n",
    "https://gymnasium.farama.org/environments/toy_text/taxi/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a3f59f-c131-46c4-9cd5-19620e8b50c9",
   "metadata": {},
   "source": [
    "# Reward table\n",
    "\n",
    "Each state of the agent needs a reward for the action that can be potentially be taken.\n",
    "\n",
    "| State                       | BUY | SELL | HOLD |\n",
    "| --------------------------- | --- | ---- | ---- |\n",
    "| **GOLD_WITHIN_FAIT**        | +1  | −3   | 0    |\n",
    "| **GOLD_WITHOUT_FAIT**       | +1  | −3   | 0    |\n",
    "| **HIT_GOLD_FAIT**           | +3  | −3   | 0    |\n",
    "| **DEATH_WITHIN_FAIT**       | −3  | −3   | 0    |\n",
    "| **DEATH_WITHOUT_FAIT**      | −3  | −3   | 0    |\n",
    "| **GOLD_WITHIN_ASSET**       | −3  | −0.5 | +0.1 |\n",
    "| **DEATH_WITHIN_ASSET**      | −3  | −0.5 | +0.1 |\n",
    "| **DEATH_WITHOUT_ASSET**     | −3  | −0.5 | +0.1 |\n",
    "| **ABOVE_TAKE_PROFIT_ASSET** | −3  | +3   | +0.1 |\n",
    "| **BELOW_STOPLOSS_ASSET**    | −3  | +2   | −1   |\n",
    "| **HIT_DEATH_WITH_ASSET**    | −3  | +3   | +0.1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a073d5-bf46-4f2d-a4c8-150e55d97b44",
   "metadata": {},
   "source": [
    "## Learning formula:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha \\Big[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)\\Big]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9f8477-bfe5-44d1-909e-b385ae598752",
   "metadata": {},
   "source": [
    "# MACD Trading Strategy\n",
    "\n",
    "- compare a slow, and fast moving average, to see if there is upward or downward momentum.\n",
    "- Golden Cross: When the fast moving average crosses over the slow average.\n",
    "- Death Cross: When the slow moving average crosses below the fast average.\n",
    "- Stop-Loss: A price level where you automatically exit a trade to limit losses.\n",
    "- Risk-Reward: The ratio between how much you’re willing to lose and how much you aim to gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb8aea-835f-4164-b33d-cf67c75213f7",
   "metadata": {},
   "source": [
    "## 1. List all of the possible states and actions of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60d0aee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent actions: [0, 1, 2], Agent states: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "from libs.indicators import Indicator\n",
    "from libs.indicators import MACD\n",
    "\n",
    "ACTION_HOLD = 0\n",
    "ACTION_BUY = 1\n",
    "ACTION_SELL = 2\n",
    "USD = \"USDT\"\n",
    "ASSET = \"BTC\"\n",
    "\n",
    "actions = [ ACTION_HOLD, ACTION_BUY, ACTION_SELL ];\n",
    "\n",
    "mMACD = MACD( pair=[USD,ASSET], stopLoss=-1.0, riskReward=\"1:3\");\n",
    "\n",
    "print(\"Agent actions: {}, Agent states: {}\".format(actions,mMACD.getStates()));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50e864-91a3-4aba-aafb-828de963364a",
   "metadata": {},
   "source": [
    "## 2. Initialise Q Table, and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84700201-d213-4907-a21f-d017f224e7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "q_table = np.random.uniform( low=-0.01, high=0.01, size=( len( mMACD.getStates() ) , len( actions ) ) );\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "discount_factor = 0.9\n",
    "\n",
    "exploration_probability = 0.3\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv( \"./assets/BTC_USDT-4h.csv\" ,index_col=0,parse_dates=True)\n",
    "df = df.copy()\n",
    "df.index.name = 'Date'\n",
    "\n",
    "mState = {};\n",
    "work_index = 0;\n",
    "\n",
    "def initState():\n",
    "    global mState, work_index, q_table, mMACD\n",
    "    mMACD = MACD( pair=[USD,ASSET], stopLoss=-1.0, riskReward=\"1:3\");\n",
    "    mMACD.prep( df );\n",
    "    q_table = np.random.uniform( low=-0.01, high=0.01, size=( len( mMACD.getStates() ) , len( actions ) ) );\n",
    "    work_index = 0;\n",
    "    mState[\"last\"] = df.index[0];\n",
    "    mState[\"start\"] = df.index[0];\n",
    "    mState[\"closeLast\"] = df.iloc[0][\"close\"]\n",
    "    mState[\"openLast\"] = df.iloc[0][\"open\"]\n",
    "    mState[\"volumeLast\"] = df.iloc[0][\"volume\"]  \n",
    "    mState[\"trades\"] = 0\n",
    "    mState[\"holding\"] = USD\n",
    "    mState[\"balancesBefore\"] = { USD: 0, ASSET: 0 }\n",
    "    mState[\"start\"] = 100\n",
    "    mState[\"balances\"] = { USD: 100, ASSET: 0 }\n",
    "    \n",
    "initState();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e255bb0b-8c92-4099-9489-59403d71a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _agent_work( i ):\n",
    "    global mState\n",
    "    mMACD.work( df , i );\n",
    "    if i > 0:\n",
    "        mState[\"closeLast\"] = df.iloc[i-1][\"close\"]\n",
    "        mState[\"openLast\"] = df.iloc[i-1][\"open\"]\n",
    "        mState[\"volumeLast\"] = df.iloc[i-1][\"volume\"]\n",
    "\n",
    "def _usdToAsset( spotUSD, holdingUSD  ):\n",
    "    return holdingUSD / spotUSD;\n",
    "    \n",
    "def _assetToUSD( spotUSD, holdingAsset  ):\n",
    "    return holdingAsset * spotUSD;\n",
    "\n",
    "def _enact( action, i):\n",
    "    global mState\n",
    "    try:\n",
    "        price = (df.iloc[i][\"close\"] + df.iloc[i][\"open\"]) / 2\n",
    "        holding = mState[\"holding\"]\n",
    "\n",
    "        if holding == USD:\n",
    "            if action != ACTION_BUY:\n",
    "                return False\n",
    "            prev_usd = mState[\"balances\"][USD]\n",
    "            asset = _usdToAsset(price, prev_usd)\n",
    "            mMACD.buy(price)\n",
    "            mState[\"holding\"] = ASSET\n",
    "            mState[\"balances\"][USD] = 0\n",
    "            mState[\"balances\"][ASSET] = asset\n",
    "            mState[\"balancesBefore\"][USD] = prev_usd\n",
    "            mState[\"trades\"] += 1\n",
    "\n",
    "            return True\n",
    "        \n",
    "        if holding == ASSET:\n",
    "            if action != ACTION_SELL:\n",
    "                return False\n",
    "            prev_asset = mState[\"balances\"][ ASSET ]\n",
    "            usd = _assetToUSD(price, prev_asset)\n",
    "            mMACD.sell(price)\n",
    "            mState[\"holding\"] = USD\n",
    "            mState[\"balances\"][ ASSET ] = 0\n",
    "            mState[\"balances\"][ USD ] = usd\n",
    "            mState[\"balancesBefore\"][ ASSET ] = prev_asset\n",
    "            mState[\"trades\"] += 1\n",
    "            return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "def _calculate_reward(action, i ):\n",
    "    if df.size == 0:\n",
    "        raise ValueError(\"No Data Frame\");\n",
    "    if i >= df.size:\n",
    "        return 0;\n",
    "    return mMACD.getReward( action, df, i );\n",
    "\n",
    "def _q_learning_update( state, action, reward, next_state):\n",
    "    global q_table\n",
    "    if q_table.size == 0:\n",
    "        raise ValueError(\"No Q table\");\n",
    "    q_predict = q_table[state, action]\n",
    "    q_target = reward + discount_factor * np.max(q_table[next_state])\n",
    "    q_table[state, action] += learning_rate * (q_target - q_predict)\n",
    "\n",
    "def _work( enact=True):\n",
    "    global work_index\n",
    "    try:\n",
    "        if df.size == 0:\n",
    "            raise ValueError(\"No Data Frame\");\n",
    "        \n",
    "        while work_index < df.shape[0] - 1:\n",
    "            _agent_work( work_index );\n",
    "            state = mMACD.getState( df , work_index );\n",
    "            \n",
    "            if None == state: \n",
    "                work_index += 1;\n",
    "                continue;\n",
    "                \n",
    "            if np.random.rand() < exploration_probability:\n",
    "                action = np.random.randint( len( actions ) )\n",
    "            else:\n",
    "                action = np.argmax( q_table[state])\n",
    "\n",
    "            if work_index < df.size:\n",
    "                # Update the Q-table\n",
    "                next_time_step = work_index + 1\n",
    "                reward = _calculate_reward( action, next_time_step )\n",
    "                next_state = mMACD.getState( df , next_time_step );\n",
    "                _q_learning_update( state, action, reward, next_state)\n",
    "                # Enact the action\n",
    "                if reward > 0 and enact == True:\n",
    "                    _enact( action , work_index );\n",
    "                work_index = next_time_step\n",
    "    except:\n",
    "        traceback.print_exc();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad79aac-efc7-4aeb-bce3-071ae5ba4977",
   "metadata": {},
   "source": [
    "## work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "448d9245-b6ed-4d85-9a1e-ae95299c75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train( episodes = 1 ):\n",
    "    global work_index, mState\n",
    "    print(\"Training\");\n",
    "    statePreUSD = mState[\"balances\"][ USD ];\n",
    "    statePreAsset = mState[\"balances\"][ ASSET ];\n",
    "    spot = ( mState[\"closeLast\"] + mState[\"openLast\"] ) / 2\n",
    "    if statePreUSD == 0:\n",
    "        statePreUSD = _assetToUSD( spot, statePreAsset );\n",
    "    if statePreAsset == 0:\n",
    "        statePreAsset = _usdToAsset( spot, statePreUSD );\n",
    "\n",
    "    for episode in range( episodes ):\n",
    "        is_last = (episode == episodes - 1)\n",
    "        mState[\"trades\"] = 0;\n",
    "        work_index = 0;\n",
    "        _work( enact=is_last );\n",
    "\n",
    "    state_post_usd = mState[\"balances\"][USD]\n",
    "    state_post_asset = mState[\"balances\"][ASSET]\n",
    "    spot = (mState[\"closeLast\"] + mState[\"openLast\"]) / 2\n",
    "    if state_post_usd == 0:\n",
    "        state_post_usd = _assetToUSD(spot, state_post_asset)\n",
    "    if state_post_asset == 0:\n",
    "        state_post_asset = _usdToAsset(spot, state_post_usd)\n",
    "    def percent_change(old, new):\n",
    "        return 0 if old == 0 else ((new - old) / old) * 100\n",
    "    usd_pct = percent_change(statePreUSD, state_post_usd)\n",
    "    asset_pct = percent_change(statePreAsset, state_post_asset)\n",
    "    print(\"\\n\" + \"=\" * 45)\n",
    "    print(f\"{'':7} | {'USD':>12} | {'ASSET':>12}\")\n",
    "    print(f\"{'START':7} | {statePreUSD:12.5f} | {statePreAsset:12.5f}\")\n",
    "    print(f\"{'END':7} | {state_post_usd:12.5f} | {state_post_asset:12.5f}\")\n",
    "    print(f\"{'% Δ':7} | {usd_pct:11.2f}% | {asset_pct:11.2f}%\")\n",
    "    print(\"=\" * 45)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dfbf957-da3a-4f88-8ce2-3350291826ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "\n",
      "=============================================\n",
      "        |          USD |        ASSET\n",
      "START   |    100.00000 |      0.00215\n",
      "END     |   1383.37115 |      0.01429\n",
      "% Δ     |     1283.37% |      564.52%\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "initState()\n",
    "_train( episodes=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd9f1b4e-c58f-4a54-bb70-205b27aad98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "\n",
      "=============================================\n",
      "        |          USD |        ASSET\n",
      "START   |    100.00000 |      0.00215\n",
      "END     |    593.58289 |      0.00613\n",
      "% Δ     |      493.58% |      185.14%\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "initState()\n",
    "_train( episodes=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d98c1190-23b1-4958-b79d-a3d456b1c886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "\n",
      "=============================================\n",
      "        |          USD |        ASSET\n",
      "START   |    100.00000 |      0.00215\n",
      "END     |    942.30296 |      0.00973\n",
      "% Δ     |      842.30% |      352.65%\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "initState()\n",
    "_train( episodes=5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fc1de-fd38-49de-8da0-1e81016a0cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
